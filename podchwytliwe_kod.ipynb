{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import gspread # chatGPT sugestion\n",
    "from oauth2client.service_account import ServiceAccountCredentials # chatGPT sugestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definitions\n",
    "\n",
    "def main():\n",
    "    model_name, output_file, samples_number, batch_size, prompt_ver, sheet_url, sheet_name = load_parameters()\n",
    "\n",
    "    df = load_dataset(sheet_url, sheet_name) # Downloading data from Google Sheets\n",
    "    \n",
    "    if samples_number is None:\n",
    "        samples_number = df.shape[0]  # Number of samples for analysis\n",
    "\n",
    "    # Model parameters\n",
    "    llm_params = {  \n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"do_sample\": False\n",
    "    }\n",
    "    if output_file is None:\n",
    "        output_file = \"answers_prompt.csv\"\n",
    "\n",
    "    # Calculating responses for the selected model\n",
    "    df_subset = df[:samples_number]  \n",
    "    df_with_answers = calculate_for_model(model_name, df_subset, llm_params, prompt_ver, batch_size)\n",
    "\n",
    "    # Saving the results to Google Sheets and a local file\n",
    "    save_to_google_sheets(df_with_answers, sheet_url, sheet_name)\n",
    "    df_with_answers.to_csv(output_file, index=False)\n",
    "\n",
    "def load_parameters():\n",
    "    parser = argparse.ArgumentParser(description=\"Script that generates answers for the benchmark.\")\n",
    "    parser.add_argument('--model_name', type=str, required=True, help='Name of the model to calculate the answers')\n",
    "    parser.add_argument('--samples_number', type=int, default=None, help='Number of samples to generate answers')\n",
    "    parser.add_argument('--output_file', type=str, default=\"generated_answers.csv\", help='Name of output file to save generated answers')\n",
    "    parser.add_argument('--batch_size', type=int, default=1, help=\"Batch size\")\n",
    "    parser.add_argument('--prompt_ver', type=int, default=1, help=\"Versions of generation prompt\")\n",
    "    parser.add_argument('--sheet_url', type=str, required=True, help=\"URL to the Google Sheets document\")\n",
    "    parser.add_argument('--sheet_name', type=str, required=True, help=\"Name of the sheet within the Google Sheets document\")\n",
    "    args = parser.parse_args()\n",
    "    return args.model_name, args.output_file, args.samples_number, args.batch_size, args.prompt_ver, args.sheet_url, args.sheet_name\n",
    "\n",
    "def load_dataset(sheet_url, sheet_name):\n",
    "    # Authentication with Google Sheets API\n",
    "    scope = [\"https://spreadsheets.google.com/feeds\", \"https://www.googleapis.com/auth/drive\"]\n",
    "    credentials = ServiceAccountCredentials.from_json_keyfile_name(\"credentials.json\", scope)\n",
    "    client = gspread.authorize(credentials)\n",
    "\n",
    "    # Reading data from Google Sheets\n",
    "    sheet = client.open_by_url(sheet_url).worksheet(sheet_name)\n",
    "    data = sheet.get_all_records()\n",
    "    \n",
    "    # Conversion to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "def calculate_for_model(model_name, df, llm_params, prompt_ver=1, batch_size=1):  # suggested changes from chatGPT\n",
    "    \"\"\"\n",
    "    Przetwarza odpowiedzi dla jednego modelu LLM na podstawie podanego zestawu danych.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Nazwa modelu do użycia.\n",
    "        df (pd.DataFrame): DataFrame zawierający dane wejściowe z kolumną 'Opinia'.\n",
    "        llm_params (dict): Parametry generowania odpowiedzi dla modelu.\n",
    "        prompt_ver (int): Wersja promptu (domyślnie 1).\n",
    "        batch_size (int): Liczba danych przetwarzanych w jednym batchu.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame z dodaną kolumną zawierającą wygenerowane odpowiedzi.\n",
    "    \"\"\"\n",
    "    print(f\"Przetwarzanie modelu: {model_name}\")\n",
    "    \n",
    "    # Initialization of tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = load_model(model_name)\n",
    "    pipe = load_pipe(model, tokenizer)\n",
    "\n",
    "    # The name of the resulting column for the answer\n",
    "    answer_column = f'answer_{model_name.split(\"/\")[-1]}'\n",
    "\n",
    "    # Measuring processing time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Generating responses in batches\n",
    "    answers = []\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        # Pobranie danych dla batcha\n",
    "        batch = df['Opinia'].iloc[i:i + batch_size].tolist()\n",
    "        \n",
    "        # Downloading data for batcha\n",
    "        batch_answers = generate_answers_batch(batch, tokenizer, pipe, llm_params, prompt_ver=prompt_ver)\n",
    "        answers.extend(batch_answers)\n",
    "\n",
    "        # Information on progress\n",
    "        print(f\"Przetworzono batch {i // batch_size + 1}/{(len(df) - 1) // batch_size + 1}\")\n",
    "\n",
    "    # Adding an answer column to the DataFrame\n",
    "    df[answer_column] = pd.Series(answers, index=df.index[:len(answers)])\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Czas przetwarzania dla modelu {model_name}: {end_time - start_time:.2f} sekund\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def save_to_google_sheets(df, sheet_url, sheet_name):\n",
    "    # Authentication with Google Sheets API\n",
    "    scope = [\"https://spreadsheets.google.com/feeds\", \"https://www.googleapis.com/auth/drive\"]\n",
    "    credentials = ServiceAccountCredentials.from_json_keyfile_name(\"credentials.json\", scope)\n",
    "    client = gspread.authorize(credentials)\n",
    "\n",
    "    # Opening Google Sheets\n",
    "    sheet = client.open_by_url(sheet_url).worksheet(sheet_name)\n",
    "\n",
    "    # Replace the contents of the sheet with new data\n",
    "    sheet.clear()\n",
    "    sheet.update([df.columns.values.tolist()] + df.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '_main_':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
