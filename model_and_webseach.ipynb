{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "import pandas as pandas\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TextStreamer\n",
    "import torch\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import requests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zmienne do usunięcia przy przejściu na skrypt\n",
    "dataset_URL = \"To Do - wstawić link do datasetu\"\n",
    "model_id = \"To Do - wstawić id modelu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the main function\n",
    "def main():\n",
    "    '''\n",
    "    The script's main function.\n",
    "    '''\n",
    "    print(\"Getting started...\")\n",
    "    # loading the dataset\n",
    "    df = pd.read_csv(dataset_URL)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary definitions\n",
    "def load_dataset(dataset_URL):\n",
    "    \"\"\"\n",
    "    Loads data from CSV and removes redundant columns\n",
    "\n",
    "    Args:\n",
    "        dataset_url (str): URL of the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Loading dataset\n",
    "    df = pd.read_csv(dataset_URL)\n",
    "\n",
    "    # Removing excessive columns (e.g. 'Unnamed')\n",
    "    df.drop(columns=df.columns[df.columns.str.contains('^Unnamed')], inplace=True)\n",
    "    \n",
    "    return df\n",
    "def load_model_and_tokenizer(model_name):\n",
    "    \"\"\"\n",
    "    Loads model in 4-bit quantization mode along with its tokenizer.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the model from Hugging Face.\n",
    "\n",
    "    Returns:\n",
    "        model (AutoModelForCausalLM): Quantized model.\n",
    "        tokenizer (AutoTokenizer): Tokenizer for the model.\n",
    "\n",
    "    \"\"\"\n",
    "    # Configuring BitsAndBytes for 4-bit quantization\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    # Loading the model with quantization\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    # Loading the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# def create_pipeline(model, tokenizer):\n",
    "#     \"\"\"\n",
    "#     Creates a pipeline for the model and tokenizer.\n",
    "\n",
    "#     Args:\n",
    "#         model (transformers.PreTrainedModel): Model.\n",
    "#         tokenizer (transformers.PreTrainedTokenizer): Tokenizer.\n",
    "\n",
    "#     Returns:\n",
    "#         transformers.Pipeline: Pipeline.\n",
    "#     \"\"\"\n",
    "#     # Creating pipeline\n",
    "#     pipeline = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "#     return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def querry_llm(df, pipeline):\n",
    "    \"\"\"\n",
    "    Generates answers for the questions in the 'question' column and saves them in a new column.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with 'question' column.\n",
    "        pipe: Pipeline of the language model.\n",
    "        pipeline (transformers.Pipeline): Pipeline.\n",
    "\n",
    "    Returns:\n",
    "       Returns:\n",
    "        pd.DataFrame: Updated DataFrame with a new column \"llm_answer\".\n",
    "    \"\"\"\n",
    "    # Querying the model\n",
    "    if \"question\" not in df.columns:\n",
    "        raise ValueError(\"DataFrame must contain a 'question' column.\")\n",
    "    # Iterate through questions and generate answers\n",
    "    answers = []\n",
    "    for inx, question in enumerate(df[\"question\"], start=1):\n",
    "        print(f\"Processing question {inx}/{len(df)}...\")\n",
    "        response = pipeline(question)\n",
    "        generated_text = response[0][\"generated_text\"]\n",
    "        print(f\"Response: {generated_text}\")\n",
    "\n",
    "        answers.append(generated_text)\n",
    "    # Adding answers to the DataFrame\n",
    "    df[\"llm_answer\"] = answers\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_web(df, num_results):\n",
    "    \"\"\"\n",
    "    Searches the web using DuckDuckGo and returns the top results.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with 'question' column.\n",
    "        num_results (int): Number of top results to return.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame with a new column \"web_results\" (list of URLs).\n",
    "    \"\"\"\n",
    "    if \"question\" not in df.columns:\n",
    "        raise ValueError(\"DataFrame must contain a 'question' column.\")\n",
    "\n",
    "    web_results = []\n",
    "\n",
    "    for idx, question in enumerate(df[\"question\"], start=1):\n",
    "        print(f\"Searching the web for question {idx}/{len(df)}: {question}\")\n",
    "\n",
    "        search_url = f\"https://duckduckgo.com/html/?q={question}\"\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(search_url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Downloading the first `num_results` results\n",
    "            raw_links = [a[\"href\"] for a in soup.select(\".result__url\")][:num_results]\n",
    "            print(f\"Found {len(raw_links)} results.\")\n",
    "\n",
    "            # Convert relative URLs to absolute URLs\n",
    "            links = []\n",
    "            for link in raw_links:\n",
    "                if link.startswith(\"//duckduckgo.com/l/?uddg=\"):\n",
    "                    cleaned_link = urllib.parse.unquote(link.split(\"uddg=\")[-1])\n",
    "                    cleaned_link = cleaned_link.split(\"&\")[0]\n",
    "                    links.append(cleaned_link)\n",
    "                elif link.startswith(\"http\"):\n",
    "                    cleaned_link = link.split(\"&\")[0]\n",
    "                    links.append(cleaned_link)\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching search results for '{question}': {e}\")\n",
    "            links = []\n",
    "        \n",
    "        web_results.append(links)\n",
    "        time.sleep(2)  \n",
    "\n",
    "    df[\"web_results\"] = web_results\n",
    "    return df\n",
    "\n",
    "def fetch_page_content(url):\n",
    "    \"\"\"\n",
    "    Fetches the content of a webpage.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL of the webpage.\n",
    "\n",
    "    Returns:\n",
    "        str: Content of the webpage.\n",
    "    \"\"\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    \n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Remove unnecessary sections\n",
    "        for tag in soup(['script', 'style', 'header', 'footer', 'nav', 'aside']):\n",
    "            tag.decompose()\n",
    "\n",
    "        # Extracting the main content of the page\n",
    "        text = ' '.join(soup.stripped_strings)\n",
    "\n",
    "        return text[:10000]  # Limit of 10,000 characters\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Błąd pobierania strony {url}: {e}\")\n",
    "        return None\n",
    "        \n",
    "def extract_web_content(df):\n",
    "    \"\"\"\"\n",
    "    Retrieves article content from pages stored in 'web_results' and adds them to DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with a 'web_results' column containing lists of URLs.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with a new column “extracted_text” (list of article content).\n",
    "    \"\"\"\n",
    "    if \"web_results\" not in df.columns:\n",
    "        raise ValueError(\"DataFrame must contain a 'web_results' column.\")\n",
    "\n",
    "    extracted_texts = []\n",
    "\n",
    "    for idx, urls in enumerate(df[\"web_results\"], start=1):\n",
    "        print(f\"Pobieranie treści dla zapytania {idx}/{len(df)}...\")\n",
    "        page_texts = [fetch_page_content(url) for url in urls if url]  \n",
    "        extracted_texts.append(page_texts)\n",
    "        time.sleep(2)  \n",
    "\n",
    "    df[\"extracted_text\"] = extracted_texts\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
